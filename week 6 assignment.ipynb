{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "PART TWO: TASK 1 (WEB CAM)"
      ],
      "metadata": {
        "id": "qQKlbut3q36x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "train_edge_model\n"
      ],
      "metadata": {
        "id": "A49omJpRlIb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pathlib\n",
        "import os\n",
        "\n",
        "# --- Configuration Parameters ---\n",
        "# Standard image size for model input (height, width)\n",
        "IMG_SIZE = (160, 160)\n",
        "# Batch size for datasets\n",
        "BATCH_SIZE = 32\n",
        "# Number of training epochs\n",
        "EPOCHS = 3\n",
        "# Number of classes in the dataset\n",
        "NUM_CLASSES = 5\n",
        "# Filename for the quantized TensorFlow Lite model\n",
        "TFLITE_MODEL_NAME = 'model_quantized.tflite'\n",
        "# Filename for saving the Keras model\n",
        "KERAS_MODEL_PATH = 'image_classifier_keras.h5'\n",
        "\n",
        "# --- Step 1: Data Preparation ---\n",
        "print(\"--- 1. Preparing Data ---\")\n",
        "\n",
        "# Download and extract the flower photos dataset\n",
        "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
        "data_dir_base = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)\n",
        "data_dir = pathlib.Path(data_dir_base) / 'flower_photos'\n",
        "\n",
        "# Create training and validation datasets from directory\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=123,\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=123,\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "# Verify identified class names\n",
        "class_names = train_ds.class_names\n",
        "print(f\"\\nVerification: Found {len(class_names)} classes: {class_names}\")\n",
        "\n",
        "# Optimize dataset loading with cache, shuffle, and prefetch\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "\n",
        "# --- Step 2: Model Training (MobileNetV2 Transfer Learning) ---\n",
        "print(\"\\n--- 2. Training Keras Model (MobileNetV2 Transfer Learning) ---\")\n",
        "\n",
        "# Define MobileNetV2 preprocessing function (scales pixels to [-1, 1])\n",
        "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
        "\n",
        "# Load pre-trained MobileNetV2 base model (ImageNet weights, no top layer)\n",
        "base_model = tf.keras.applications.MobileNetV2(\n",
        "    input_shape=IMG_SIZE + (3,),\n",
        "    include_top=False,\n",
        "    weights='imagenet'\n",
        ")\n",
        "\n",
        "# Freeze base model weights for transfer learning\n",
        "base_model.trainable = False\n",
        "\n",
        "# Global average pooling layer to reduce feature dimensions\n",
        "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "\n",
        "# Prediction layer with units equal to number of classes (outputs logits)\n",
        "prediction_layer = tf.keras.layers.Dense(len(class_names))\n",
        "\n",
        "# Construct the full Keras model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Lambda(preprocess_input, input_shape=IMG_SIZE + (3,)),\n",
        "    base_model,\n",
        "    global_average_layer,\n",
        "    prediction_layer\n",
        "])\n",
        "\n",
        "# Compile model with Adam optimizer, sparse categorical crossentropy, and accuracy metric\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)\n",
        "\n",
        "# Save the trained Keras model\n",
        "model.save(KERAS_MODEL_PATH)\n",
        "\n",
        "# Calculate and print saved Keras model size\n",
        "original_size_mb = os.path.getsize(KERAS_MODEL_PATH) / (1024 * 1024)\n",
        "print(f\"Keras Model Saved. Original size (H5 approx): {original_size_mb:.2f} MB\")\n",
        "\n",
        "\n",
        "# --- Step 3: Model Conversion to TensorFlow Lite with Quantization ---\n",
        "print(\"\\n--- 3. Converting to TFLite with Quantization ---\")\n",
        "\n",
        "# Initialize TFLiteConverter from the Keras model\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "# Apply Dynamic Range Quantization for size reduction and speed\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the converted TFLite model to a file\n",
        "tflite_model_path = TFLITE_MODEL_NAME\n",
        "with open(tflite_model_path, 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "# Calculate and print TFLite model size and reduction\n",
        "quantized_size_mb = os.path.getsize(tflite_model_path) / (1024 * 1024)\n",
        "print(f\"TFLite Model Saved at: {tflite_model_path}\")\n",
        "print(f\"Quantized size: {quantized_size_mb:.2f} MB\")\n",
        "print(f\"Size Reduction: {100 * (1 - quantized_size_mb / original_size_mb):.2f}%\")\n",
        "\n",
        "\n",
        "# --- Step 4: Validate TFLite Model Accuracy (Inference) ---\n",
        "print(\"\\n--- 4. Validating TFLite Model Accuracy ---\")\n",
        "\n",
        "# Load TFLite model and allocate tensors\n",
        "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensor details\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Initialize Keras Accuracy metric for TFLite validation\n",
        "tflite_accuracy_metric = tf.keras.metrics.Accuracy()\n",
        "\n",
        "def evaluate_tflite_model(interpreter, dataset, accuracy_metric):\n",
        "    \"\"\"Evaluates the TFLite model's accuracy on a given dataset.\"\"\"\n",
        "    for x, y in dataset:\n",
        "        input_data = tf.cast(x, input_details[0]['dtype'])\n",
        "        for i in range(input_data.shape[0]):\n",
        "            # Prepare single image for inference\n",
        "            input_tensor = input_data[i:i+1]\n",
        "            interpreter.set_tensor(input_details[0]['index'], input_tensor)\n",
        "            interpreter.invoke()\n",
        "            # Get output logits\n",
        "            output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "            # Determine predicted class\n",
        "            predicted_class = np.argmax(output_data, axis=1)\n",
        "\n",
        "            # Update accuracy metric\n",
        "            accuracy_metric.update_state(y[i].numpy(), predicted_class)\n",
        "\n",
        "    return accuracy_metric.result().numpy()\n",
        "\n",
        "# Run evaluation on the validation dataset\n",
        "tflite_accuracy = evaluate_tflite_model(interpreter, val_ds, tflite_accuracy_metric)\n",
        "\n",
        "print(f\"\\nDeployment Step Complete!\")\n",
        "print(f\"Final Accuracy Metrics (on Validation Set):\")\n",
        "print(f\" * Keras Model Validation Accuracy (Last Epoch): {history.history['val_accuracy'][-1]:.4f}\")\n",
        "print(f\" * TFLite Quantized Model Accuracy: {tflite_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "GF8zydPGMQBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tflite_model_verification"
      ],
      "metadata": {
        "id": "eRAfT_RLl9pS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- TFLite Model Verification ---\n",
        "# This section verifies that the TFLite model can be loaded and perform a dummy inference.\n",
        "\n",
        "# Load TFLite model and allocate tensors\n",
        "interpreter = tf.lite.Interpreter(model_path=TFLITE_MODEL_NAME)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensor details (shape, data type)\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "print(f\"Model Input Shape: {input_details[0]['shape']}\")\n",
        "print(f\"Model Input Dtype: {input_details[0]['dtype']}\")\n",
        "\n",
        "# Create dummy input tensor (zeros) matching model's expected shape and dtype\n",
        "dummy_input = np.zeros((1,) + IMG_SIZE + (3,), dtype=np.float32)\n",
        "\n",
        "# Set dummy input, invoke interpreter, and retrieve output\n",
        "interpreter.set_tensor(input_details[0]['index'], dummy_input)\n",
        "interpreter.invoke()\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "print(f\"\\nDummy Inference Successful.\")\n",
        "print(f\"Output shape (Logits): {output_data.shape}\")\n",
        "# Print predicted class index for the dummy input\n",
        "print(f\"Predicted Class Index (Argmax): {np.argmax(output_data)}\")"
      ],
      "metadata": {
        "id": "1ktjpkzPTYVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "single_image_inference"
      ],
      "metadata": {
        "id": "S0uAK0-mmAxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Class names for interpreting model output\n",
        "CLASS_NAMES = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n",
        "\n",
        "def run_face_inference(image_path, interpreter, class_names):\n",
        "    \"\"\"Loads an image, preprocesses it, and runs TFLite inference to predict its class.\n",
        "\n",
        "    Args:\n",
        "        image_path (str or pathlib.Path): Path to the image file.\n",
        "        interpreter (tf.lite.Interpreter): The loaded TFLite model interpreter.\n",
        "        class_names (list): List of class names corresponding to model outputs.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Load and Resize Image\n",
        "    img = Image.open(image_path).resize(IMG_SIZE)\n",
        "    img_array = tf.keras.utils.img_to_array(img)\n",
        "    input_data = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    # 2. Preprocessing (MobileNetV2 style: scales pixels to [-1, 1], type float32)\n",
        "    preprocessed_input = tf.keras.applications.mobilenet_v2.preprocess_input(input_data).astype(np.float32)\n",
        "\n",
        "    # 3. TFLite Inference\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "    interpreter.set_tensor(input_details[0]['index'], preprocessed_input)\n",
        "    interpreter.invoke()\n",
        "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "    # 4. Post-processing and Result\n",
        "    probabilities = tf.nn.softmax(output_data).numpy()[0]\n",
        "    predicted_index = np.argmax(probabilities)\n",
        "    predicted_class = class_names[predicted_index]\n",
        "    confidence = probabilities[predicted_index]\n",
        "\n",
        "    # 5. Display Result\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"Prediction (Simulated Face Recognition):\\nClass: {predicted_class} ({confidence*100:.2f}%)\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\n--- Simulation Result ---\")\n",
        "    print(f\"Model Predicted: **{predicted_class}** with **{confidence*100:.2f}%** confidence.\")\n",
        "    print(\"In a real system, this confidence value would determine if the face is recognized/unlocked.\")\n",
        "\n",
        "# --- EXECUTE THE TEST ----\n",
        "# Demonstrates `run_face_inference` using a sample image from the downloaded dataset.\n",
        "\n",
        "# Get path to 'daisy' folder and find the first JPG image\n",
        "daisy_dir = pathlib.Path(data_dir) / 'daisy'\n",
        "try:\n",
        "    test_image_path = next(daisy_dir.glob('*.jpg'))\n",
        "except StopIteration:\n",
        "    print(\"Error: No .jpg files found in the daisy directory.\")\n",
        "    test_image_path = None\n",
        "\n",
        "# Run inference if a test image is found\n",
        "if test_image_path:\n",
        "    print(f\"Successfully found test image: {test_image_path.name}\")\n",
        "    interpreter = tf.lite.Interpreter(model_path=TFLITE_MODEL_NAME)\n",
        "    interpreter.allocate_tensors()\n",
        "    run_face_inference(str(test_image_path), interpreter, CLASS_NAMES)\n",
        "else:\n",
        "    print(\"Cannot run inference test without a sample image.\")"
      ],
      "metadata": {
        "id": "GhB4R4r4Tkzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install opencv-python"
      ],
      "metadata": {
        "id": "MFJ-C2CHVW31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "webcam_test.py"
      ],
      "metadata": {
        "id": "1-hNWtC7w-Tx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "# --- Configuration Parameters (Match model training) ---\n",
        "TFLITE_MODEL_PATH = 'model_quantized.tflite'\n",
        "IMG_SIZE = (160, 160)\n",
        "CLASS_NAMES = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n",
        "\n",
        "# --- TFLite Model Setup ---\n",
        "try:\n",
        "    interpreter = tf.lite.Interpreter(model_path=TFLITE_MODEL_PATH)\n",
        "    interpreter.allocate_tensors()\n",
        "except Exception as e:\n",
        "    print(f\"Error loading TFLite model: {e}\")\n",
        "    print(f\"Make sure '{TFLITE_MODEL_PATH}' is in the current directory.\")\n",
        "    exit()\n",
        "\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
        "\n",
        "print(\"TFLite Model Loaded. Starting webcam stream...\")\n",
        "\n",
        "# --- Webcam Loop (Note: This code will not work in a typical Colab environment) ---\n",
        "# Intended for local Python environment with OpenCV installed.\n",
        "\n",
        "# Initialize video capture for default webcam\n",
        "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
        "if not cap.isOpened():\n",
        "    print(\"Error: Could not open webcam.\")\n",
        "    exit()\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # --- 1. Preprocessing Current Frame for Model Inference ---\n",
        "    input_frame = cv2.resize(frame, IMG_SIZE)\n",
        "    input_data = np.expand_dims(input_frame, axis=0)\n",
        "    preprocessed_input = preprocess_input(input_data).astype(np.float32)\n",
        "\n",
        "    # --- 2. TFLite Model Inference ---\n",
        "    interpreter.set_tensor(input_details[0]['index'], preprocessed_input)\n",
        "    interpreter.invoke()\n",
        "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "    # --- 3. Post-processing Model Output ---\n",
        "    probabilities = tf.nn.softmax(output_data).numpy()[0]\n",
        "    predicted_index = np.argmax(probabilities)\n",
        "    predicted_class = CLASS_NAMES[predicted_index]\n",
        "    confidence = probabilities[predicted_index]\n",
        "\n",
        "    # Calculate real-time performance metrics\n",
        "    end_time = time.time()\n",
        "    latency = end_time - start_time\n",
        "    fps = 1.0 / latency\n",
        "\n",
        "    # --- 4. Display Results on the Live Frame ---\n",
        "    height, width, _ = frame.shape\n",
        "    cv2.rectangle(frame, (50, 50), (width - 50, height - 50), (0, 255, 0), 2)\n",
        "\n",
        "    text = f\"Result: {predicted_class} ({confidence*100:.1f}%)\"\n",
        "    cv2.putText(frame, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)\n",
        "\n",
        "    fps_text = f\"FPS: {fps:.1f} | Latency: {latency*1000:.1f}ms\"\n",
        "    cv2.putText(frame, fps_text, (width - 250, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2, cv2.LINE_AA)\n",
        "\n",
        "    cv2.imshow('Edge AI Prototype (Press Q to Quit)', frame)\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release webcam and destroy windows\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "PR3aaGRNVqU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install ai-edge-litert"
      ],
      "metadata": {
        "id": "teVhgUVzZ1-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART TWO: TASK TWO (SMART AGRICULTURAL SIMULATION SYSTEM)"
      ],
      "metadata": {
        "id": "X9RAsH7sv7QK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class AgricultureSensorSimulator:\n",
        "    \"\"\"Generates synthetic time-series data for smart agriculture sensors over a growing period.\n",
        "\n",
        "    This simulator aims to create realistic-looking data for sensor readings\n",
        "    (soil moisture, temperature, pH) and a derived plant health index,\n",
        "    which can be used for training and testing agricultural models.\n",
        "    \"\"\"\n",
        "    def __init__(self, days=90):\n",
        "        \"\"\"Initializes the simulator with a specified number of days for data generation.\n",
        "\n",
        "        Args:\n",
        "            days (int): The total number of days for which to simulate data.\n",
        "        \"\"\"\n",
        "        self.days = days\n",
        "        self.timestamps = pd.to_datetime(pd.date_range(start='2024-05-01', periods=days, freq='D'))\n",
        "        self.soil_moisture_base = 0.55 # Volumetric Water Content (VWC)\n",
        "        self.temp_base = 25.0         # Average daily temperature (Â°C)\n",
        "        self.ph_base = 6.5            # Soil pH\n",
        "\n",
        "    def generate_data(self):\n",
        "        \"\"\"Generates synthetic time-series data for agricultural sensors.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A DataFrame with sensor data and a simulated final crop yield.\n",
        "        \"\"\"\n",
        "        # 1. Simulate Soil Moisture (cyclic + noise, clipped to realistic range)\n",
        "        moisture = self.soil_moisture_base + 0.1 * np.sin(np.linspace(0, 4*np.pi, self.days))\n",
        "        moisture += np.random.normal(0, 0.03, self.days)\n",
        "        moisture = np.clip(moisture, 0.4, 0.7)\n",
        "\n",
        "        # 2. Simulate Temperature (warming trend + daily fluctuations)\n",
        "        temp_trend = np.linspace(-2, 3, self.days)\n",
        "        temperature = self.temp_base + temp_trend + np.random.normal(0, 1.0, self.days)\n",
        "\n",
        "        # 3. Simulate Soil pH (stable with minor fluctuations, clipped)\n",
        "        ph = self.ph_base + np.random.normal(0, 0.05, self.days)\n",
        "        ph = np.clip(ph, 6.0, 7.0)\n",
        "\n",
        "        # 4. Mock Plant Health Index (PHI - growth trend + random variation, clipped)\n",
        "        phi = np.linspace(0.1, 0.95, self.days)\n",
        "        phi += np.random.normal(0, 0.08, self.days)\n",
        "        phi = np.clip(phi, 0.0, 1.0)\n",
        "\n",
        "        # Combine data into a DataFrame\n",
        "        data = pd.DataFrame({\n",
        "            'Day': range(1, self.days + 1),\n",
        "            'Soil_Moisture': moisture,\n",
        "            'Temperature': temperature,\n",
        "            'Soil_pH': ph,\n",
        "            'Plant_Health_Index': phi\n",
        "        }, index=self.timestamps)\n",
        "\n",
        "        # Calculate mock final crop yield based on PHI, moisture, and random component\n",
        "        yield_score = 50 + 40 * np.mean(phi[self.days // 2:])\n",
        "        yield_score += 10 * np.mean(moisture)\n",
        "        final_yield = yield_score + np.random.uniform(-5, 5)\n",
        "\n",
        "        return data, final_yield\n",
        "\n",
        "# --- Execution Demonstration of the Simulator ---\n",
        "# Create simulator instance for 120 days and generate data\n",
        "sensor_data, predicted_yield = AgricultureSensorSimulator(days=120).generate_data()\n",
        "\n",
        "print(\"--- Simulated IoT Sensor Data (First 5 Days) ---\")\n",
        "print(sensor_data.head())\n",
        "print(\"\\n--- Simulated Final Prediction Target ---\")\n",
        "print(f\"Target Crop Yield (Tons/Hectare): {predicted_yield:.2f}\")\n",
        "\n",
        "# --- Optional: Plotting Time-Series Data for Visualization ---\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(sensor_data.index, sensor_data['Soil_Moisture'], label='Soil Moisture (VWC)')\n",
        "plt.plot(sensor_data.index, sensor_data['Temperature'] / 30, label='Temperature (Scaled)', linestyle='--')\n",
        "plt.plot(sensor_data.index, sensor_data['Plant_Health_Index'], label='Plant Health Index (PHI)')\n",
        "plt.title('Simulated Sensor Readings Over 120 Days')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Value (Normalized/Scaled)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oBdHuRlJ8mMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Concatenate\n",
        "\n",
        "def create_hybrid_yield_model(time_steps, num_features, image_feature_vector_size):\n",
        "\n",
        "    # --- Branch 1: Sequential Sensor Data Processing (LSTM) ---\n",
        "    # Input layer for time-series sensor data (batch_size, time_steps, num_features)\n",
        "    sequential_input = Input(shape=(time_steps, num_features), name='sequential_input')\n",
        "    # LSTM layer to process sequence data and output a summary vector\n",
        "    lstm_output = LSTM(units=64, activation='relu', name='lstm_layer')(sequential_input)\n",
        "\n",
        "    # --- Branch 2: Image Features Input ---\n",
        "    # Input layer for image-derived feature(s) (batch_size, image_feature_vector_size)\n",
        "    cnn_input = Input(shape=(image_feature_vector_size,), name='cnn_feature_input')\n",
        "\n",
        "    # --- Merging and Final Prediction (Regression Head) ---\n",
        "    # Concatenate LSTM output and CNN features\n",
        "    merged_features = Concatenate(name='merge_layer')([lstm_output, cnn_input])\n",
        "\n",
        "    # Dense hidden layer for merged features\n",
        "    hidden_layer = Dense(units=32, activation='relu')(merged_features)\n",
        "    # Output layer for continuous crop yield prediction\n",
        "    output_prediction = Dense(units=1, name='yield_output')(hidden_layer)\n",
        "\n",
        "    # Construct the Keras Model\n",
        "    model = Model(inputs=[sequential_input, cnn_input], outputs=output_prediction)\n",
        "\n",
        "    # Compile the model with Adam optimizer, MSE loss, and MAE metric\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# --- Execution Demonstration of Model Creation ---\n",
        "# Define model parameters\n",
        "TIME_STEPS = 120\n",
        "NUM_FEATURES = 3\n",
        "IMAGE_FEATURES = 1\n",
        "\n",
        "# Create hybrid yield prediction model\n",
        "yield_model = create_hybrid_yield_model(\n",
        "    time_steps=TIME_STEPS,\n",
        "    num_features=NUM_FEATURES,\n",
        "    image_feature_vector_size=IMAGE_FEATURES\n",
        ")\n",
        "\n",
        "print(\"\\n--- Mock Hybrid Model Summary (Deliverable for Task 2) ---\")\n",
        "# Print model architecture summary\n",
        "yield_model.summary()"
      ],
      "metadata": {
        "id": "_mS6yK6G9AWi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}